% Survey of existing FPGA-based discrete event simulators, and their
% microarchitectural design.


For the purpose of this dissertation, it is insightful to classify FPGA-based
RTL simulators along two dimensions. For simplicity, here we consider only
single-FPGA hosts, but we note that this discussion can trivially extended to
multi-FPGA hosts. This chapter was heavily influenced by dicussion in Pellauer
and Vijayaraghavan et al. in \emph{A-Port Networks: Preserving the Timed
Behavior of Synchronous Systems for Modeling on FPGAs}~\cite{APortNetwork}.

The first dimension, is timekeeping strategy. \emph{Explicit timekeeping}~(ET)
simulators, keep track of simulation time whereas simulators with implicit
timekeeping (IT) instead rely on target-cycle count as a proxy for target time. This represents an
implementation optimization, as additional FPGA resources required track and
manage timestamps are unneeded. For this optimization to apply broadly across
the simulator, target system clocks must have fixed frequency and phase, and
all simulation events must be synchronous to these clocks.

The second dimension is control granularity. At one extreme there exist
simulators with \emph{centralized control} (CC). These track time in
a single location, the entire simulator advances in lockstep from
timestep to timestep.  Conversely, there are simulators with \emph{distributed
control}~(DC). These simulators are parallel systems of \emph{logical
processes}~(LP). Each LP locally tracks simulation time and can advance
independently. DC simulators can be coarse-grained, with a logical process
simulating core-scale components of the SoC, core of the SoC, or fine-grained,
herewit logical processes may take on the scope of a CAM, RAM, or even a
combinational circuit like a multiplexer.

Taking the product of these two dimensions produces four classes of simulator:

\begin{enumerate}
\item{Implicit Timekeeping, Centralized Control (ITCC)}: Simple MIDAS generated simulators*. FabScalar FPGA

\item{Implicit Timekeeping, Distributed Control (ITDC)}: RAMP simulators,networked MIDAS-generated simulators.

\item{Explicit Timekeeping, Centralized Control (ETCC)}: Unrealized, commerical emulators*. Example: Direct implementations of the verilog event queue.

\item{Explicit Timekeeping, Distributed Control (ETDC)}: Multi-FPGA commerical emulators*, This Work
\end{enumerate}

It is critical to note DC simulators are in effect FPGA-hosted, parallel
discrete-event simulators.  Parallel, discrete-event simulation~(PDES) has been
a vibrant area of research since the 1980s, but PDES researchers have focussed
nearly exclusively on non-FPGA hosts (including multiprocessors, GPUs,
supercomputers, networks thereof).  In the next sections, we briefly introduce
relevant PDES work, and explain how prior work in that field translates to
hosting DC full-system simulators on FPGAs. First, we explore some
CC simulator designs to help motivate the additional complexity DC simulators incur.

\section{An Iron Law For FPGA-Based Simulator Performance}

If SoC designers turn to hardware emulators for speed, it is critical
to understand how emulator design affects throughput. For IT simulations of a target
with a single clock domain, Pellauer et al. present a simple performance
equation, which like the iron-law of processor performance that inspired it,
breaks down the performance of a complete simulation into a product of terms:

\begin{equation}
    f_{sim} = \frac{cycles_{t}}{cycles_{h}} f_{fpga}
\end{equation}\label{eq:sim-perf}

\noindent Where,
\begin{flalign*}
    f_{sim} &= \text{throughput of the simulator (target cycles per second, Hz)}\\
    f_{fpga} &= \text{the clock frequency of the host FPGA (Hz)}\\
    cycles_{h} &= \text{the total number of host cycles over which the simulation executed}\\
    cycles_{t} &= \text{the total number of target cycles simulated}\\
\end{flalign*}

The right term of this equation, host frequency~($f_{fpga}$), is set by the
critical path delay of the simulator. Depending on the simulator's design, and
the target it models, this may sometimes be SoC RTL often corresponding to a
critical path in the actual implementation, but more likely, it is a path that
drives scheduling logic in the simulator itself (i.e., deciding whether to
advance forward in simulation time or fire a target clock edge).
The left term, a ratio of host-to-target cycles is a measure of the
microarchitectural efficiency of the simulator. In an FPGA prototype, this term is
effectively unity: every host clock simulates a target clock. In
a host-decoupled simulator, in practise, it is always less than one\footnote{While it is possible to simulate multiple target cycles per
host cycle by unrolling successive cycles of execution, there is no incentive
to do so globally across a simulator, as it would double resource utilization,
and likely double the critical path length of the simulator. In DC simulators
of target machines with multiple clock domains, it may make sense to do this
for relatively small LPs in the fastest clock domains, if the simulator is
rate-limited on those LPs.}. Given this, Pellauer et al. tend to the
more-intuitive recipricol of this term and call it FPGA-cycles-to-Model-cycles
Ratio~(FMR). We give it below.

\begin{equation}
    FMR = \frac{cycles_{h}}{cycles_{t}}
\end{equation}\label{eq:fmr}

As an example, consider a 5-read, 3-write register file modelled by an LP that
uses a 2-ported block RAM. If the LP statically schedules all 8 accesses, it
will have an FMR of 4. Conversly, a more clever design that dynamically
schedules accesses only if the ports are used could have lower FMR. In
simulations where the register file is lightly used the FMR of the LP could
approach unity, whereas when it is heavily used it would approach the static
limit of 4. Unless it is statically defined, FMR can only be usefully defined
as an average over the execution of a particular simulation. Continuing with
this example, one could gang-together multiple BRAMs or LUTRAMs to build a more
highly-ported RAM structure that could model more port accesses per cycle.
While this could reduce the statically scheduled FMR from 4 to as low as 1, it
comes at the expense of FPGA resources and, potentially, longer simulator
critical path adversely affecting the second term of \ref{eq:sim-perf}.

For this to apply to DC simulators, we can modify the equation above to use the FMR of
the LP that has executed the fewest number of target cycles in the system. We
note though, that when LPs may advance only a bounded number of cycles ahead of
slower LPs in the system for sufficiently long simulation, the equation above
returns approximately the same result for all LPs.

\section{Centrally Controlled~(CC) Simulators}

It is natural, when first building a simulator, to attempt to coordinate time
globally across the FPGA. It is simpler to implement; there are fewer potential
sources of causality errors, as there aren't different pieces of the simulator at different points in simulated time; and it is considerably
easier to capture snapshots of the simulated system at particular points in
time. In a system where different components of the target may take different,
or even dynamically changing number of host cycles to model, there are two
common approaches.  The first, is what A-Port Networks, as \emph{Unit-Delay
Simulation}. Here the simulator is granted a fixed $N$ host-clock-cycles to
compute each target-cycle, where $N$ is selected to equal the largest possibly
latency a sub-block may take to model its piece of the target~(its FMR is $N$). While simple to
implement, this will result in lost host-cycles, if in some target cycles the
full $N$ cycle allocation is unneeded.  We also note that while it's
relatively easy to provide tight bounds for models of many on-chip
blocks it may be more difficult for I/O devices, which may require large $N$
in the worst case, but far less on average.

A natural optimization to make, is to allow sub-blocks to signal when they have
finished computing.  The centralized controller aggregate done-signals from all
blocks in the system and steps the simulator only once all have been
asserted~(i.e., a wide and-reduction).  This is called \emph{Dynamic-Barrier
Synchronization}. While this removes wasted host idle cycles (FMR can fall
below $N$), it introduces new control signals that must be routed to a central
location on the FPGA. This adds considerable wire delay to paths that likely
lie on the simulators critical path. This problem is exacerbated in modern
FPGAs, like the VU9P devices used in EC2 F1, which are composed of multiple
dies~\footnote{In Xilinx parlance, each logic die is referred to as a
super-logic regions~(SLRs).}, as these control signals must use relatively
scarser and higher-delay inter-die interconnect. Additionally, the scarcity of inter-die interconnect
puts increased pressure on the router, and for high utilizations often results
in unrouted nets, and other related DRC errors.  To improve simulator
frequency, it is natural, perhaps even necessary, to pipeline these signals at
the expense of a fixed increase to FMR: unless simulation of target cycle can
be overlapped, FMR will increase by 1 for each additional pipeline stage. While
FPGAs have scaled in capacity, their logic delays, and thus achievable
$f_{max}$, have seen little improvement. This increase in capacity begets a
greater number of units, increasing the number of terms in the and-reduction
network and lengthening critical path of the simulator.

These trends in FPGA scaling make it natural to want to decentralize
timekeeping in the simulator.  Having more LPs, each of which have to manage a
fewer number of control signals, that are routed more locally, should improve
${f_{fpga}}$ while easing the routing and DRC challenges with centralizing
control.

\section{Software PDES}
Software PDES can be divided into two classes based on how they address
deadlock.  In \emph{Conservative} PDES, logical processes issue messages
non-speculatively, as a result, an output message that \emph{may} depend casually on a
particular input message will not be issued until that input arrives.
Conservative PDES addresses deadlock by requiring LPs to have non-zero
\emph{lookahead}. That is to say, output messages must only depend on input
messages that are at least  $t_{lookahead}$ seconds in the past. In order to gaurantee forward
progress, if a logical process is blocked on a particular input, whose latest
message is timestamped to time $t_{i}$, it can and must eventually issue a
\emph{null-message} at time $t_{i} + t_{L}$ on it's output channels. Supposing
the blocked, path does not contain a cycle of LPs no lookahead, the simulation
will advance.  Large lookahead improves simulation performance, as the number
of null-messages required to avoid deadlock increases as lookahead approaches 0.
In general, lookahead is derived from underlying properties of the system under simulation;
if sufficient lookahead cannot be captured in the underlying system, conservative PDES algorithms may
offer little benefit over sequential simulators.

The second class PDES simulation are \emph{optimistic}. Optimistic deadlock
avoidance first was presented in the Time Warp~\cite{TimeWarp}. In optimistic
pdes, lps send messages speculatively. Necessarily, LPs have mechanisms to
rollback from misspeculation and to inform downstream LPs of earlier messages
that were erronously sent. In Time Warp, LPs correct for mispeculated messages,
by sending \emph{anti-messages}, which as the physics-analogy would suggest,
\emph{annihiliate} its corresponding pair-message that was erronously sent previously.

Though a conservative vs. optimistic PDES debate raged through
the 90s and 00s, both optimisitic and conservative PDES algorithms see
contemporary use.
- Summary of where they are used.


\subsection{Considerations for FPGA-Hosted PDES}

FPGA are a unique host platform relative to those studied in the PDES
literatures, as a number of complications introduced by multiprocessors and
networked-hosts do not apply. Notably:

\begin{enumerate}
\item{FPGAs trivially support FIFO communication channels}. Unlike in
conventional PDES hosts, channels between FPGA-hosted LPs are easily made FIFO.
In a conservative PDES implementation, LPs do not need to reorder messages, and
can assume messages from the same sender arrive in monotonically increasing
time-order.

\item{Transmissions errors are rare}. FPGA-hosted channels (i.e., hardware
queues) operate reliably, and even those that cross asynchronously related host-clock-domains
can be made to operate such that LPs do not need to tolerate potentially lost messages.
\end{enumerate}

The major challenge FPGA-hosts introduce over their CPU-host counterparts is
that many of the state-of-the-art PDES algorithms are complex, making
them a challenge to implement in hardware and expensive to support in FPGA
fabric. At least one general-purpose, FPGA-hosted PDES has been built: PDES-A~\cite{PDESA}~\cite. PDES-A
implements an optimistic algorithm, over an array of processing elements~(PEs).
While it would be possible to implement RTL simulator using
PDES-A, the effective simulation capacity and throughput of such a system would be
considerably lower than FPGA-based emulators. ASIC emulation aside, it is
reasonable to assume that likely some domain-specific, FPGA-hosted PDES have been built
and published in their respective fields. One example published at FCCM by
Herboldt et. al.~\cite{MolecularFPGAPDES} to simulate molecular dynamics.
% See PDESA paper for cite

\section{ASIC Emulation As Domain-Specific PDES}

In effect, DC simulators are domain-specific, FPGA-hosted PDES for doing ASIC emulation.
As an application domain, ASIC emulation has a number of properties that make FPGAs well-suited for one another.;

The first, most obvious reason is the reason FPGAs are used as ASIC prototyping
platforms, they are a good implementation sub LPs. In the case of a RTL block with a single clock, a
simple LP implementation clock-gates the block it models, and adds one or more state
machines to control when to fire the clock, and enqueue and dequeue messages
from other LPs. When an LP clock fires, what would be thousands, possibly
millions or billions, of events in a software simulator is computed
concurrently in a single host cycle. We expand on various LP implementations in
the next section.

Second, since FPGAs offer high-bandwidth, and critically, low-latency
interconnect between LPs that is tailored to the target, it is possible to
decompose an ASIC into closely-coupled LPs that synchronize on a per-cycle
basis. High bandwidth interconnect between LPs removes any need to compress, or
otherwise optimize, transmission between LPs, reducing simulator complexity.
Hundreds, even thousands, of cycle-by-cycle traces of messages can be trivally
moved between LPs on the same FPGA, a feat that is all but impossible to
achieve on a CPU host. LPs can be directly connected to one another with
hardware queues, which in the general case, have a single host cycle of
"transmission" delay. In some cases, flow-through queues or wire connections
may be used to allow combinational transmission of messages. This permits
decomposing an ASIC into LPs that are closely, even combinationally, coupled in
the target, without grossly affecting simulation performance.



%  RAMP papers are domain-specific simulators

%  Explain the assumptions. SIngle clock-domain.
%  Explain the general structure of these simulators.
%  Explain what a target formalism is:
%    -> A means of describing an ASIC as agraph of logical LPs. Different formalism impose different rules on the LPs.
%  Talk about deadlock avoidance
%    -> segue to APorts
%    -> MIDAS' target formalism.
%       -> Restricted form of APortNetwork
%       -> FAME1 transform 
%    -> FAME tranform.
%    -> LI-BDN
%        -> Give LP multiple rules, allow them to evaluate 









Two dimensions of classification.

Timekeeping Strategy
% Implicit (Cycle-based) vs Explicit (Time-stamped)

Simulation Control (Spectrum)
% Centralized vs Distributed

Synchronization Schemes \& Control Algorithms
%Synchronization
% Formalisms for Distributed, implicitly time-kept RTL Simulators

MIDAS FAME-1 Transform \& Simulator Microarchitecture

