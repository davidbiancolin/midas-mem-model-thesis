This work draws from two different schools of system-level simulation thought.
In the first third of this chapter, we discuss system-level simulation in the
context SoC design and verification. Second, we explore academic work computer
architecture to accelerate microarchitectural simulations of future
microprocessors. Finally, we introduce precusor work done at Berkeley
Architecture Research group over the past decade, that attempts to build better
full-system simulators, or in industrial parlance, hardware emulators, for SoC
verification by applying techniques from these microarchitecture simulators as
automatic RTL transformations.

To avoid confusion when speaking of computers simulating computers, the
literature commonly makes a distinction between the \emph{target}, the system
being simulated, and the \emph{host} or \emph{host-platform}, the system
executing the simulation. The host-platform is often not a single machine but
a collection of interconnected machines, which may include CPUs,
GPUs, and FPGAs.

\section{A Tour of Full-System Simulation for SoC Design}

Simulation is central in performing three fundamental tasks of SoC design.

\begin{enumerate}

    \item \textbf{Prototyping:} ``What thing should we
        build?" Prototyping serves as a means to rapidly evaluate different
        design points with an imperfect model of a proposed design.

    \item \textbf{Verification:} ``Did we build the thing right?" Verification
        serves to check, or prove, that a particular implementation
        correctly executes.

    \item \textbf{Validation:} ``Did we build the right thing?" Validation
        serves to show that the implementation fulfills the objectives set out
        for the system.

\end{enumerate}

Both prototyping and verification can be applied at all levels of the design
hierarchy.  For example, given a specification of the system into which an
accelerator is integrated, one could prototype different design points and
verify an implementation of that accelerator. Validation, however, seeks to
answer a system-level question that spans the entire computing stack.  The
surest way to validate a system is not in simulation, but at-speed with a
physical prototype or the final product itself. However, waiting for a silicon
prototype pushes validation late into the design cycle making it challenging or
impossible to pivot the design of the system based on validation results. To
perform \emph{pre-silicon} validation, a fast and accurate full-system
simulator is required.

Here, SoC designers are confronted with a fidelity-performance-cost trade-off,
and are forced use multiple different simulation technologies at different
points in this space. A summary of these technologies can be found in
Table~\ref{tbl:full-system-simulation-tech}.

\subsection{CPU-Hosted Simulation for Prototyping}\label{UArchSWSim}

Architecture-level simulators such as QEMU\cite{qemu}, which model the system at the
instruction-set-architecture level and include as limited set of standard
device models for I/O, are fast and inexpensive as they run on conventional CPUs.
When augmented with simple timing models, they are ideal for doing initial system prototyping, as
these models can be quickly modified and recompiled.
However, as these timing models become more complex, they become more
challenging to validate, and cruically, the throughput of these simulators rapidly declines.

Continuing in the direction of increasing fidelity, microarchitecture-level
simulators such as Gem5~\cite{gem5}, and MARSSx86~\cite{marssx86}. are CPU-hosted 
simulators that provide configurable, "cycle-level" timing models of a complete systems, including CPU pipelines,
caches, and off-chip memory systems.  These simulators can run target workloads at hundreds of KIPS, but are
often much slower in practice when employing more detailed or custom models. This
makes it practically impossible to run complete workloads, such as
multi-threaded Java applications or SPECint2006~\cite{spec} with its reference
inputs. Here, a common remedy is to employ statistical sampling
techniques~\cite{smarts} to fast-forward to the region of interest on an architecture-level simulator, before
executing O(100M) instructions at the desired fidelity.

While this approach has well-acknowledged shortcomings~\cite{gem5error},
judicious use of cycle-level simulators can be an appropriate vehicle for
doing initial system prototyping. For radical proposals that involve
aggressive microarchitectural changes or traverse multiple layers of the
computing stack, this approach is often inadequate (particularly for workloads that
are multithreaded, or are long-running and irregular, for which it is diffcult to collect 
meaningful samples without peturbing the system under evaluation, such as
managed-language workloads~\cite{MicroSimPanel}).


\subsection{CPU-Hosted Simulation for Verification}

Since the aforementioned simulation techniques use abstract models of the
target system, they are useless for system verifaction and validation once
implementation begins. (In fact, those models will need to be validated against the
implementation as it is completed). Instead, here designers use CPU-hosted
simulators that faithfully represent the implementation at a particular
abstraction level.

For simulating digital components of the SoC, a Register Transfer Level~(RTL) ,
like Synopsys VCS, or Verilator, is the tool of choice. Broadly speaking, RTL
simulators model state elements and the combinational functions that update
them at cycle boundaries. Values on wires transition instantiously, and in
general no delay through launching registers, combinational circuits is
modelled. Supposing the underlying digital abstraction holds, RTL simulation
ideal for doing dynamic verification of a digital circuit. Small blocks compile
in seconds, while complete SoCs can be compiled in 1s to 10s of minutes. RTL
simulators are relatively easy to debug, as they provide complete visibility
over the state of the design over the entire duration of a simulation. The
cost~(\$) of an RTL simulator comes mainly from licensing fees, as simulators
run on standard servers or desktop CPUs, though in many cases open-source RTL
like Verilator can be used instead. Ultimately, the largest challenge in using
CPU-hosted RTL simulator is that they have poor simulation throughput for large
designs. Completely SoCs execute at hundreds to less than 1 Hz -- much too slow
to do verification for all but small inputs (e.g., checking system boot), or
for doing full-system validation.

It's important to note at this point that, higher-fidelity software simulation
of the design is commonly used after the SoC is synthesized and implemented in
a particular process technology.  These simulations include combinational,
wire, and parasitic delays, and can generally include more detailed models of
analog components of the system. The additional fidelity only exercabates the
throughput limitation of CPU-based simulation.

To do effective full-system validation and dynamic verification, considerably
faster simulation throughput is required. While there are many techniques that
can improve throughput on CPUs, such as multithreading, relaxing or restricting
the the timing sematics of the design language, ultimately the abundant
fine-grained, often bit-level, parallelism of RTL simulation cannot exploited
by multiprocessors to overcome the 6+ order of magnitude slow down over a
silicon prototype.

\subsection{Hardware-Accelerated Full-System Simulation -- FPGA Prototyping}

To build simulators that execute a rates closer to a silicon prototype, designers turned
to fine-grained parallel hardware to simulate SoCs.  The earliest form of
hardware-accelerated full-system simulation emerged in the 1980s, and used
programmable logic devices, specifically FPGAs, to directly implement the
design. This is known as \emph{FPGA prototyping}.

Modern FPGA prototypes directly implement the SoC on one more
more FPGAs, often with a custom board design that may include peripherals
identical to those that would be deployed in the final system.  FPGA prototypes
are fast: small prototypes that fit in a single FPGA execute at ones to hundreds of
MHz, while larger prototypes, which must be partitioned across multiple FPGAs,
simulate at hundreds of kHz~\cite{nehalemprototype, atomprototype}.
Often, FPGA prototypes are inexpensive enough that they be can readily
duplicated and distributed across hardware and software engineering teams.

Relative to software simulation, prototypes have greater fixed costs (to buy, design, and/or license the 
prototyping platform), but more critically, are difficult to use:
\begin{enumerate}
    \item Poor design visibility makes it difficult to debug failing systems. Users must instantiate
        FPGA specific debugging hardware which provides only a limited set of
        signals for small window of time, as these tools consume considerable
        FPGA resources.  If the bug is not found on the first iteration, the
        process must be repeated: the design must be resynthesized with a new
        sset of sampled signals.

    \item Long compile times (ones to tens of hours) make it difficult to
        iterate about a design point, and lengthens the aforementioned the debug cycle.

    \item Large designs must be partitioned across multiple FPGAs either
        manually, or with specialized tools. This makes the prototype hardware
        more expensive and decreases simulation throughput.

    \item Many ASIC structures, such as multi-ported RAMs and clock generators,
        cannot be synthesized to FPGA fabric so must be replaced with an FPGA
        equivalent.

    \item FPGA-specific I/O models are required to build out a complete system. Using hardened
        IP on the FPGA may not be a good model of the target system. Doing software co-simulation of IO
        often reduces simulation throughput.

    \item Prototypes are not natively deterministic making it difficult to
        reproduce certain classes of system failure, especially those that
        involve IO.

    \item Prototypes require complete RTL implementation of the design
\end{enumerate}

These limitations make FPGA prototypes unproductive for doing full-system
verification. However, in most cases FPGA prototypes are the fastest available
full-system RTL model the target and so are used extensively for
pre-silicon software development and for regression testing.

\subsection{Hardware-Accelerated Full-System Simulation -- Hardware Emulation}

Hardware emulation aims to provide productive full-system verification by
marrying the speed FPGA prototypes with the usability software simulators.
Hardware emulators tend to be expensive to license and run -- millions of
dollars per unit -- making them a precious commodity that must be carefully
shared across a company.

Each of the three major CAD vendors offer hardware emulation solutions.
Mentor Veloce~\cite{Veloce} and Cadence Palladium~\cite{Palladium} are the historical market leaders in
hardware emulation, with the two often swapping positions with the release of updated
emulation platforms, with Synopsys ZeBu~\cite{ZeBu} rounding out the offerings.
Differences in the implementations of these three emulators put them at
different points in the cost-usability-performance space -- we describe these three strategies in the following sections.

\subsubsection{ASICs - Logic Processor Arrays}

Perhaps the earliest form of hardware emulation traces back to IBM research's
Yorktown Simulation Engine~(YUE, \cite{YSEHardware}) and it's industrial-strength successor project
the Engineering Verification Engine~(EVE, \cite{EngineeringVerificationEngine}). These machines consisted of an array
of small processors which would simulates pieces of the DUT. Processors were
specialized for simulating logic (Logic Processors) and memory (Array
Processors), and were interconnected with high-radix crossbars (256 x 256 in
these early machines). A compiler~\cite{YSESoftware} would partition, map, and schedule
the DUT onto this array. Both EVE and YUE could do gate-level simulation in zero-delay
or unit-delay (every gate propogation incurs a constant delay) modes. Cyclist~\cite{Cyclist}
is a recent academic work that further explores this approach; it uses a
homogenous array of RISC-V cores attached in a 2-D mesh network.

Cadence's Palladium~\cite{Palladium} emulators derive from EVE (which was for a time sold by QuickTurn Design Systems under the name CoBALT~\cite{CoBALT}, before they
were acquired by Cadence. Relative to
competing emulation solutions, Palladiums have lower capacity per unit, but the
fastest compile times. Palladiums also have highest power consumption and must be
water-cooled -- unlike the competing offerings.


\subsubsection{Custom FPGAs}
While a EVE-like processor arrays can provide radically improved compilation speeds since
the compilation problem is fundamentally simpler than running FPGA place and
route, one natural alternative is to modify conventional FPGAs for emulation.
This might involve adding dedicated hardware to improve debugability (e.g., to make it
possible to capture full-visibility waveforms), adding I/O multiplexing
hardware to ease the FPGA-partitioning problem~(i.e., a hardware implementation of Virtual Wires~\cite{VirtualWires}), and modifying the programmable
fabric to better match the resources and interconnect required ASIC designs.

This is the approach taken by Mentor Graphics's Veloce~\cite{Veloce} emulation platforms.
Veloce emulators appear to have better capacity than Palladium, but slower
compile times. Veloce emulators use less power and are air-cooled.

\subsubsection{Commercial-Off-The-Shelf FPGAs}

Hardware emulation large total-cost of ownership is driven to some degree
because the leading emulators are both power-hungry and use custom silicon. To
skirt some of the costs of those platforms, a third alternative that sees
industrial use is to use large, commericial-off-the-shelf FPGAs but rely on
custom tooling and system packaging to improve usability. Synopsys's
ZeBu~\cite{ZeBu} plaform does percisely this by leveraging the largest
available Xilinx FPGAs available at the time of its release (Virtex Ultrascale
VU440s~\cite{ZeBu}). At time of writing, ZeBu provides the highest simulation
throughput and the lowest power consumption but has a reduced debug feature set
relative to other emulators.

In it's use of COTS FPGAs, Synopsys ZeBu is most similar to the work presented herein. 

\begin{sidewaystable}
\begin{center}
\resizebox{\textwidth}{!}{%
    \begin{tabular}{|p{0.1\textwidth}|p{0.1\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|}
    \hline
        \textbf{Technology} & \textbf{Examples} & \textbf{Speed}\newline(Relative to Silicon) &
        \textbf{Fixed Cost} \newline(\$ per simulator) &
        \textbf{Variable Cost}\newline(\$ per target-second) & \textbf{``Compile" Time}\newline(Hours)  \\
    \hline
    \hline
        Arch SW Simulator & QEMU \newline Spike & $10^{-1}$ &
        Free + $10^3$ & \TODO{} & 0 - 0.1 \\
    \hline
        $\mu$Arch SW Simulator & Gem5 \newline MARSSx86 & $10^{-6} - 10^{-4}$ &
        Free + $10^{3}$ & \TODO{} & 0 - 0.1 \\
    \hline
        RTL Simulation & VCS \newline Verilator  & $10^{-7} - 10^{-4}$ &
        $10^{3} /seat/yr + 10^{3}$ \newline $10^{3}$ & \TODO{} & $10^{-2} - 10^{-1}$ \\
    \hline
        Single-FPGA prototype & FPGA-zynq & $10^{-2} - 10^{-1}$ & 2495~(ZC706) \newline 495~(Zedboard) &%
        \TODO{10W (ZC706)} & 0.5 - 1 (FPGA-zynq)\newline $10^{0} - 10^{1}$ \\
    \hline
        Multi-FPGA prototype & \cite{nehalemprototype}, \cite{atomprototype} \newline Protium S1  &
        $10^{-4} - 10^{-3}$ & \TODO{} & \TODO{} & \TODO{} \\
    \hline
        Hardware Emulation & Palladium Z1 & $10^{-3} - 10^{-2}$ &
        $10^{6}$ & \TODO{$10^{-2}$} & 140 MG/hr \\
    \hline
        Silicon Test-chip & \TODO{} & 1 & $10^{5} - 10^{7}$ & \TODO{$10^{-5} - 10^{-4}$} & $10^3 - 10^4$ \\
    \hline
\end{tabular}}
\end{center}
    \caption{Constrasting different technologies for building full-system
    simulators; ordered approximately from top-to-bottom in descending
    fidelity. We define ``compile time" to be the time it takes to make one
    design iteration less the time spent in simulation and implementing a design
    change; the time to generate a simulator from a specification of the
    target.}
\label{tbl:full-system-simulation-tech}
\end{sidewaystable}%

\section{Hardware-Accelerated Microarchitecture Simulation}

While the aim of this work is ultimately to produce more cost-effective hardware emulation for 
pre-silicon silicon verification validation, as mentioned previously, it attempts to do so by drawing
from technologies developed in academia to accelerate cycle-level microarchitecture simulation~(see Section~\ref{UArchSWSim}).

The first, most obvious way to try to accelerate these simulations is to
parallelize simulations over multiprocessors or networks of workstations.
Notably, the Wisconsin Wind Tunnel~(WWT, \cite{WisconsinWindTunnel}), relying
on a window-based parallel discrete-event simulation engine to manage the
synchronization. For simulators like the WWT, and more recent works such as Graphite~\cite{Graphite}, to acheive achieve
good performance, they must reduce the synchronization overhead between
partitions of the design by modelling the target more abstractly, or by
introducing extra target-latency between partitions.  While this may be
appropriate for building coarser models of processor architectures, it's
ineffective for accelerating RTL simulations, which in the general require
per-cycle synchronization.

In order to build simulators that were both fast and close-to-cycle-accurate,
many academics turned to FPGAs, which by the 90s and early 00s proven themselves as effective
vehicles for ASIC prototyping and emulation. Instead of directly implementing an
ASIC, instead, microarchitectural models could be written in RTL and
hosted on an FPGA. Early instances of this approach include \TODO{refs}, the bulk 
of the research in this domain can be attributed to the RAMP project~\cite{RAMP}.

\subsection{Research Accelerator For Multiple Processors (RAMP)}

The RAMP project began in 2005 driven by the realization that future computer
microarchitectures were going to seek parallelism beyond ILP, as the end of
Dennard scaling would necessarily make single-threaded performance improvements
more difficult to attain. The goal was to develop a shared full-system
simulation infrastructure for the computer architecture community which would
be better suited to study future thread-parallel systems of 64 - 1024
processors, than traditional software simulators. Collaborators included UT
Austin, CMU, UC Berkeley, University of Washington, Stanford, and MIT.  While
the RAMP project ultimately failed to produced a unified simulation
infrastructure, differences in the the simulators produced by the member
institutions each uniquely advanced the state-of-the-art.

At the onset of the project three initial, motholithic prototypes were built,
each was designed to model a different class of target.
RAMP Red, later known as ATLAS~\cite{ATLAS}(Stanford) was designed to study transactional-memory-based chip-multiprocessors. It supported up to 8 PowerPC cores.
RAMP Blue~\cite{RAMPBlue}(UC-Berkeley) was designed to simulate
large-scale distributed-memory message-passing machines and used Xilinx
Microblaze softcores to prototype the target system. Partitioned over 21
BEE2 boards, RAMPBlue could simulate a system with as many as 1008 cores.
Finally, RAMP White~\cite{RAMPWhite}(UT Austin) models cache-coherent
shared-memory processors. It supports both PowerPC (when using a Xilinx
host with a hardened PowerPC 405 core), and 32-bit SPARCV8 (soft core) targets.
Each of these prototypes used the same host platform (the Berkeley Emulation
Engine 2), and were initially constructed using shared libraries and a common
specification language called the RAMP design language.

Other, mostly later, projects tied to RAMP abandoned the shared infrastructure
and explored different simulator design styles.
ProtoFlex~\cite{ProtoFlex}~(CMU) was an architecture-level simulator that
demonstrated 16-way host-multithreading of a single FPGA-hosted functional
model.  ProtoFlex could switch between FPGA-hosted and CPU-hosted modes via
\emph{transplantation}.  FAST~\cite{FAST}~(UT Austin) was a cycle-accurate
x86 simulator which leveraged a split CPU-hosted functional and FPGA-hosted
timing model. RAMPGold~\cite{RAMPGold}(UC Berkeley) used FPGA-hosted timing
and functional models with 64-way host-multithreading to model a larger
target on a single FPGA.  To model a datacenter-scale target,
DIABLO~\cite{Diablo} leveraged RAMP Gold's multithreading to simulate 3072
servers on 24 FPGAs.  Finally HASim~\cite{HASim}(MIT) also used FPGA-hosted
timing and functional models, but provided more detailed pipeline and
memory hierarchy models. Other work studied partitioning targets over
multiple FPGAs. \cite{LIFPGADesign} showed that by partitioning HAsim over
two FPGAs, they could model eight times as many cores, due to improved
resource sharing between virtual instances.

Contemperanously, other groups not associated with the RAMP project explored
using FPGAs for microarchitecture simulation.  One notable example is
DART~\cite{DART}, an FPGA-based Network-on-Chip simulator, that leverages
decoupling and multithreading like many RAMP simulators.

\subsection{FAME Taxonomy}

* One output of the RAMP project was the FAME (FPGA Architecture Model
Execution (FAME)~\cite{FAME} taxonomization of FPGA-accelerated simulation work,
which distills many of the contributions of the works above into three,
RAID-like dimensions: host-decoupling (FAME-1), abstract RTL vs concrete RTL
(FAME-2), and multithreading (FAME-4).

\subsection{FAME-1: Host-Target Decoupling}\label{sec:fame1}

In host-decoupled FPGA simulators, a target-cycle of simulation
executes over multiple, possibly variable, number of FPGA-host cycles. In contrast, an
FPGA prototype executes a single target-cycle on every FPGA-host cycle. With
this technique, ASIC structures that map inefficently to FPGA fabric may be replaced
with optimized-for-FPGA structures that take more host cycles to execute, but save
FPGA resources and improve host-cycle time.  One classic optimization replaces
multi-ported register files and CAMs with a dual-ported BRAMs pumped over
multiple cycles.  Additionally, host-decoupling permits the simulator to
tolerate variable latencies in the host-platform without sacrificing simulator
performance or changing the target-time behavior of the simulation.
Nearly all academic FPGA-accelerated simulators are FAME-XX1 simulators.
Unlike FPGA prototypes, commerical FPGA-based emulators are necessarily host decoupled to
make them deterministic, and to support a plethora of features.
We expand on mechanisms for implementing host-decoupling in Section~\ref{sim:fame1-abstractions}

\subsection{FAME-2: Abstract RTL}

In an abstract-RTL FPGA-accelerated simulation, components of the simulator do
not model the implementation RTL exactly. Abstraction permits simplifying
components of the simulator, trading simulation fidelity for FPGA-resource
savings. Additionally, abstract models can be made reconfigurable in ways the
implementation RTL cannot. Most academic FPGA-accelerated simulators are either
partially or completely abstract-RTL simulators.  In contrast, commerical
hardware emulators and FPGA prototypes are concrete -- though some ASIC features
may need to be replaced with an equivalent model provided by the emulation or
prototyping platform.

\subsection{FAME-4: Multithreading}

In a multithreaded FPGA-accelerated simulation, multiple virtual instances of a
block or module within the target are simulated using a single physical
datapath on the FPGA. The target state is duplicated according to the number of
virtual instances, and a scheduler selects which virtual instance should be
simulated in a given host-cycle. ASIC logic tends to be expensive when mapped
to FPGA fabric; in FPGA prototypes, designs tend to be logic~(LUT) constrained,
which leaves much of the FPGA's embedded BRAM left unused.  Multithreading
improves the mapping efficiency of the target, by reusing the expensive logic
over multiple copies of target state which can be mapped into abundant FPGA BRAMs and registers.
HASim~\cite{HASim} and RAMPGold~\cite{RampGold} are examples of simulators that
employ multithreading. To the best of our knowledge, commerical hardware
emulators and FPGA prototypes do not using mulithreading schemes.

\subsection{RAMP Retrospective}

Ultimately, RAMP-style FPGA-accelerated microarchitecture simulation failed to take off
in the computer architecture community. There are a number of technical explanations for this, many of which
derive from the aforementioned challenges with using FPGAs to prototype ASICs.

Open-source software simulators can run on machines researchers already had access to desktops,
whereas FPGA-based simulators requires expensive hardware. For instance, the early RAMP prototypes all used
the BEE2 board. Using smaller, more inexpensive boards would come at the expense of reduced simulation capacity
and thus required the extensive resource-optimizations to support simulating research-worthy target designs.

Reproducability: since these simulators were tied to particular FPGA host
platforms, other researchers would need to purchase the same host to reproduce
publish results. Even if reserachers had access to their own FPGAs, they
generally could not run a simulator designed for a different host on their own
FPGAs without modification to simulator.

Model Complexity: This was crux of the RAMP project; designers of RAMP simulators have said that
designing models for these simulators is often more difficult than writing the RTL implementation itself.
Consider a processor pipeline: to model detailed "cycle-accurate" behavior of that pipeline
the model design must write RTL that contains much of the complexity inherent to the actual pipeline's design in order
to properly capture the right hazards which can profoundly affect the processors performance.
In order to support modeling a space of different processor designs, either at compile time by generating different model RTL, or at runtime
by reconfiguring the simulator, they must add still more complexity.
In order to provide reasonable simulation capacity, multi-cycle resource optimizations must applied to the model,
adding still more complexity, and making the model fundamentally more difficult to debug than an FPGA prototype, since
the design is decoupled in simulation time (i.e., in a given host cycle, parts of the simulator may be at target cycle $t$, while
others may be at cycles $t+1$, $t-1$) and there is additional scheduling logic through the design.
Finally, just like their software counterparts, these models, if they are to be trusted, must be validated against a real implementation. 

%
%\section{FAME-1 Target Dataflow Abstractions}\label{sim:fame1-abstractions}
%
%* Another product of the RAMP project and follow on works, were FPGA-friendly abstractions 
%for describing target designs so that they could be easily host-decoupled.
%
%* At a high level, these absractions represent the target as a dataflow graph of actors. The execution
%of this graph defines a deterministic simulation of the target it represents. These actors can be hosted
%
%* In essence this represents a simplification of conservative Parallel Discrete Event
%Simulation, we will cover this in a later chapter.
%
%in software or on the FPGA -- he
%
%* RAMP model
%  * Used in RAMPGold
%
%* APort Networks
%  * Used in HASim
%
%* Deadlock Avoidance
%
%* LI-BDN


\section{Recent Work at Berkeley}

At the end of the RAMP project it was clear that writing performance models for
FPGAs, was not going to be tractable without major breakthroughs in improving
FPGA usability. Since pushing an actual implementation through a EDA flow is
required to meaniningfully, assess a designs QoR, notably it's critical path
delay -- effort spent writing FPGA performance models would be better spent
writing the implementation. If RTL design could be made more productive,
computer architecture reserachers may be more willing to conduct
microarchitecture studies using realizable RTL designs instead of software
simulators. Once SoC implementations are available, these designs could be
transformed into RAMP-like simulators using compiler transformations. Unlike
prior RAMP designs, these simulators would actually represent area-optimized
hardware-emulators of the design -- much like a commerical hardware emulator,
they would exactly represent the input design's behavior, and so would not need
to be validated against silicon. The compiler would be called MIDAS, as it would transform
a chip design into [RAMP]Gold.

This was one motivations that drove research done by the Berkeley Architecture
Research group under the ParLab, ASPIRE Lab, and ADEPT labs.
Chisel~\cite{Chisel}, an RTL design language hosted in Scala, allows designers
to specify rich hardware generators, by leaning on the host-language to provide
powerful metaprogramming features not avaliable in SystemVerilog or VHDL.
Instead of performing parameter sweeps in a software model, with a generator,
the user explores the design space by elaborating and evaluating multiple
design \emph{instances}. The Rocket-Chip SoC generator\cite{RocketChip} is best
known Chisel generator: it generates complete RISC-V SoCs including
core-pipelines, private caches, uncores and common periphery devices. Rocket
Chip lets the user stitch together near-arbitrary networks of disperate devices
using it's diplomacy~\cite{Diplomacy} library, developed by SiFive, which leverages Scala's type
system to provide intelligent, resource optimized system integration. The
Berkeley Out-of-Order Machine (BOOM)~cite{BOOM} and Hwacha vector-fetch
processor~\cite{Hwacha} generators provide additional core IP that can be
integrated into a Rocket Chip SoC.

In parallel to the development of these generators, FIRRTL(Flexible
Intermediate Representation For RTL~\cite{FIRRTL}) was designed to support
writing resuable compiler transformations on generated instances. FIRRTL's
Scala-based compiler infrastructure basis for MIDAS, Golden Gate, and this work.

\subsection{Strober and MIDAS}

The first iteration of this FAME-style emulator compiler derived from the
Strober~\cite{Strober} energy modelling project. Since SoC-scale gate-level
simulations run much to slowly to generate workload-driven power estimates,
Strober leveraged an FPGA-based simulator to rapidly advance to randomly
selected points in simulated time. Here a state and limited I/O trace snapshot would be
captured, so they could be replayed in the slower gate-level simulator which
could then be used to drive a conventional power estimation flow. By selecting an apporpriate number of samples,
average power disappation for the SoC can be estimated within a desired error bound.

Strober, predating FIRRTL, modified Chisel's backend to transform the instance RTL to:
\begin{enumerate}
    \item Host-decouple input RTL, in essence to gate the target clock, in order to halt simulation time. This is called
        a \emph{FAME-1 transformation}.
    \item Inject a scan-chain to read out target state, and add I/O trace buffers. This enables replaying the state snapshot in either
        RTL or gate-level simulation.
    \item Elaborate a simulation wrapper, additional RTL to control the simulator.
\end{enumerate}

Strober simulators are co-hosted over an FPGA and an CPU. A \emph{driver} process, writes to memory mapped registers
on the FPGA to control simulation advance and to initiate state snapshot capture.

In parallel. Jack Koenig in late 2015 I began developing MIDAS. We wrote a
primitive FAME-1 transform, and started buidling out a library memory system
models~(this would later become FASED~\cite{FASED}).  Seeing an opportunity to
reuse work, we merged these two projects. MIDAS, as presented at
CARRV2018~\cite{MIDAS}, retained all of Strober's features, while vastly
improving simulation performance, made it
easier to support co-simulation of other library models.

MIDAS was the basis for a number of publications. Dessert, leveraged Strober's
state snapshotting features to preform \emph{ganged-simulation}, where one
instance of a simulator runs ahead to detect a simulation error either via an
assertion firing, or by detecting a commit-log mismatch between the target
processor and an online golden model. The master instance provides instructs
the lagging slave instance, to capture a state snapshot in advance of the the
error.  Using Strober's replay feature, a full visibility waveform of the
failing target design can be generated. While commerical emulators provide rich
sets of state-snapshotting features -- DESSERT's differs in that uses two
simulators, to let the simulation advance at full-throughput to the point of
failure.

Simmani~\cite{Simanni} revisted power estimation, but instead of using complete
state snapshots, injected statistically selected set of performance counters
into design. These counters could provide a dynamic estimate of total system
power disappation.

\subsection{FireSim}

One of the research goals of the ASPIRE lab was to study new architectures for
warehouse-scala computers.  he lab's proposed design, FireBox~\cite{FireBox},
was an early example of a disaggregated machine -- it would rely on high
bandwidth, photonic networking made possible with relatively inexpensive
silicon-integrated photonics, a focus of earlier work done by the lab.  To
simulate FireBox, we build FireSim -- which interconnected MIDAS-generated
simulators, with a distributed, CPU-hosted network model to build a
cycle-accurate warehouse-scale computer simulation. Much like MIDAS attempts to
automate, FireSim builds on Diablo, and relies extensively on automation to overcome usability challenges.
Instead of using a custom-host platform, FireSim uses the publich cloud, specifically Amazon Web Servce's, and requests as many FPGA (to host Rocket
Chip derived simulators) and compute nodes (to host software switch models) as
required to host the simulator. FireSim's \emph{manager} can dynamically spin
up and teardown simulators deployed to EC2 to make it easy to coordinate
simulations of arbitrary size, or to run multiple simulations in parallel.

Since the ISCA publication, FireSim has evolved to become a general-purpose hardware emulation environment
for Rocket-Chip-based SoCs. It not only hosts the FAME compiler, but additionally provides:
\begin{enumerate}
    \item Device libraries for modelling periphery devices commonly intergrated into Rocket Chip SoCs. Including a block device, UART,
        and tether model in addition a NIC model used to performed networked simulation.
    \item The \emph{manager} to automate both running simulations (which includes spinning up and initializing EC2 instances, programming FPGAs, running simulations, and collecting
        simulation results. and building simulator bitstreams.
    \item FireMarshal a utility to automate building linux distributions for
        running on RISC-V based platforms, including FireSim.
    \item Many of the most used RTL IP libraries in the Rocket-Chip ecosystem to enable the user to build out interesting systems.
\end{enumerate}


FireSim has been used both in academic and industry. At Berkeley, FireSim was
the basis for FirePerf~\cite{FirePerf}, which introduced improved instruction
tracing, and non-invasive perforance counter integration akin to that available
on commerical emulators, to perform rapid hardware-software co-design of the
NIC and the linux networking stack. These features have since been integrated
into mainline FireSim. FireSim has been used as basis for a number of
performance evaluations.  Commerically, FireSim has seen use at Intesivate,
Esperanto, and SiFive. FireSim is also being used a plaform for RISC-V systems in DARPA's
FETT bug bounty program~\cite{FETT}.

More recently, we've released Chipyard~\cite{Chipyard} which unifies all of
BAR's SoC design tools and IP under in a single environment. In Chipyard,
FireSim strictly becomes a library for doing hardware emulation -- many
standalone utilities previously hosted in FireSim, such as FireMarshal have
been hoisted up into into Chipyard,

\section{Motivations for MIDAS II}

As adoption of FireSim started to increase, it was clear that limitations with
MIDAS prevented it from seeing more widespread use. It's existing simple FAME-1
transform supported systems with only a single clock domain, this precludes
simulating any realistic system, and made it challenging to validate FireSim
against existing RISC-V silicon. Additionally, capacity challenges with EC2
FPGAs made it difficult to simulate larger systems, notably the testchips the
group was designing concurrently. Without the ability to do multi-FPGA
partitioning, and without any automated application of RAMP-style
optimizations, FireSim's was hamstrung to supporting only relatively small
SoCs. Taken together, restrictions on the scale of the design and structures present
the RTL MIDAS could accept, hamstrung FireSim to being a limited, albeit
open-source, hardware emulation for primitive Rocket Chip-based SoC designs.

To address these two concerns, Albert Magyar and I redesigned MIDAS, and called
the new FAME compiler Golden Gate. Before we expand on the initial
implementation of Golden Gate, we first review different target formalisms and
implementation strategies for building FPGA-hosted discrete event simulators.

%\section{Segue}
%The ideal full-system simulator -- sufficient for both academic and industrial
%uses -- would be inexpensive, fast and as accurate as desired throughout the
%whole design process; it would always possible to run the software stack on a
%model of the target hardware at speeds fast enough for software development.
%Initially, this simulator could be used for system-level prototyping and design
%space exploration, but as desired, more detailed models or RTL implementations
%of target components would be integrated. Ultimately, once all of the target
%RTL has been integrated, the simulator subsume the role of an FPGA prototype.
%Academics need not completely implement an SoC: they may simply stop adding
%fidelity to the simulation once they are content with the quality of their
%results.
%
%Presently, FPGAs are the only commercial off-the-shelf (COTS) technology
%capable of supporting fast, scalable, cycle-accurate simulation. Thus, we
%believe any attempt to build this ideal simulator must necessarily use FPGAs.
%However, this requires a more flexible perspective of how FPGAs can be deployed as
%simulation \emph{accelerators} and not merely RTL emulation devices as they are
%used in FPGA prototypes.

%\section{Defining FPGA-Accelerated Simulation}
%
%The distinction between FPGA-accelerated simulation and FPGA prototyping can be
%nebulous: we argue that FPGA prototyping represents a narrow subset of the larger
%space of FPGA-accelerated simulation. Key to understanding the distinction is
%first, think of simulation as any other application executing on a host, and
%second, forget that a host may be or include one or more FPGAs.
%
%A simulation is an application that takes an input and produces an output.  The
%output may be the console or file I/O of the target.  Alternatively, it may be
%a dump of the microarchitectural state of the target as it changes over the
%lifetime of the simulation.  As far as the user is concerned, the simulation
%may be optimized in any way whatsoever so long as this output is the same.
%
%Like any other application, simulations have hot spots that account for the bulk
%of their runtime. To improve runtime, the simulation may be parallelized over
%the host, either over multiple homogeneous resources, or by offloading specific
%kernels to accelerators, which may execute concurrently with the rest
%of the application.
%
%%In simulations that account for time at the cycle-level, much of this runtime
%%is dedicated to modeling the cycle-by-cycle interactions of parts of the
%%system. These models may be written in C++ or SystemC, or implemented in an HDL
%%like verilog or VHDL. Parts of the simulation may be divided into functional
%%models, like an architecture simulator, and a timing-model that does some
%%accounting of time based on a model of the microarchitecture. What's important
%%to note, is that any cycle-level model of hardware attempts to capture the
%%behavior of a fine-grained highly concurrent digital-circuit. Taken to the
%%limit, a cycle-level model is an RTL model.
%
%In FPGA-accelerated simulation, in general, we attempt to offload parts of the
%simulation that do cycle-level or cycle-accurate modeling of some part of the
%target. One way to achieve this is to dissolve the simulation spatially
%(perhaps along module boundaries): parts of the target, like an NoC or
%processor pipeline, could be offloaded to an FPGA, while models for I/O may be
%hosted on a CPU.  Alternatively, one could host a functional model of a module
%on the CPU and accelerate a detailed cycle-accurate timing-model on the FPGA
%(or vice versa).  Again, the only constraint on the implementation of the
%simulation, and thus, the implementation of FPGA-accelerated components of the
%simulation, is that its output remains the same. All other things equal, a
%faster implementation is always better.
%
%\section{Compelling Ideas in FPGA-accelerated Simulation}
%
%Researchers have devised many clever techniques for using FPGAs as simulation
%accelerators. To highlight some of these techniques, we summarize the FAME
%taxonomy~\cite{fame}, which outlines three dimensions along which
%FPGA-accelerated simulators may be categorized.
%
%\subsection{FAME-1 (XX1): Host-Target Decoupling}\label{sec:fame1}
%
%In host-decoupled fpga-accelerated simulators, a target-cycle of simulation
%executes over a variable number of FPGA-host cycles. In contrast, an
%FPGA protoype executes a single target-cycle on every FPGA-host cycle. With
%this technique, ASIC structures that map inefficently to FPGA fabric may be replaced
%with optimized-for-FPGA structures that take more host cycles to execute, but save
%FPGA resources and improve host-cycle time.  One classic optimization replaces
%multi-ported register files and CAMs with a dual-ported BRAMs pumped over
%multiple cycles.  Additionally, host-decoupling permits the simulator to
%tolerate variable latencies in the host-platform without sacrificing simulator
%performance or changing the target-time behavior of the simulation. Nearly all
%academic FPGA-accelerated simulators are FAME-xx1 simulators, though
%ProtoFlex~\cite{protoflex} is an early example.
%
%\subsection{FAME-2 (X1X): Abstract RTL}
%
%In an abstract-RTL FPGA-accelerated simulation, components of the simulator do
%not model the implementation RTL exactly. Abstraction permits simplifying
%components of the simulator, trading simulation fidelity for FPGA-resource
%savings. Additionally, abstract models can be made reconfigurable in ways the
%implementation RTL cannot. Most academic FPGA-simulators are to some degree
%abstract-RTL simulators.  For example, Strober~\cite{strober} used a
%fixed-latency pipe to model the target's DRAM subsystem~(a FAME-011 model),
%but used FAME-001 models derived directly from source RTL everywhere else.
%Conversely, RAMPGold~\cite{rampgold} was entirely abstract.
%
%\subsection{FAME-4 (1XX): Multithreading}
%
%In a multithreaded FPGA-accelerated simulation, multiple virtual instances of a
%block or module within the target are simulated using a single physical
%datapath on the FPGA. The target state is duplicated according to the number of
%virtual instances, and a scheduler selects which virtual instance should be
%simulated in a given host-cycle. ASIC logic tends to be expensive when mapped
%to FPGA fabric; in FPGA prototypes, designs tend to be logic~(LUT) constrained,
%which leaves much of the FPGA's embedded BRAM left unused.  Multithreading
%improves the mapping efficiency of the target, by reusing the expensive logic
%over multiple copies of target state which can be mapped into abundant FPGA BRAMs and registers.
%HASim~\cite{hasim} and RAMPGold~\cite{rampgold} are examples of simulators that
%employ FAME-x1x multithreading.
%
%
%\subsection{Split Timing-Functional Models}
%
%Splitting timing and functional models is ubiquitous both in software and
%FPGA-accelerated simulators, as it permits amortizing the design effort of the
%functional model over multiple timing models. This is especially crucial in
%FPGA-based simulation, where the functional model can account for the bulk of
%the complexity (e.g., 35K vs. 1K SystemVerilog LoC in RAMPGold~\cite{rampgold}. With
%host-decoupling, it is possible to host the two models on different parts of
%the host-platform: FAST~\cite{fast} hosted its functional model in software and its
%timing model in fabric (ProtoFlex did the opposite~\cite{protoflex}).
%
%For a comphrehensive survey of FPGA-accelerated simulation work we direct
%the reader to~\cite{fpgasimbook}.
%
%\section{Adoption Challenges}
%
%Despite their promise, FPGA-accelerated simulators have only been employed by
%the researchers that developed them. The failure to adopt FPGA-accelerated
%simulation methodologies more widely comes as a result of several key factors:
%
%\begin{enumerate}
%
%    \item \textbf{Availability.} Much of the early FPGA-accelerated simulation research
%        relied on boutique FPGA-host platforms like the BEE~\cite{bee2}, or
%        used custom board designs. The cost of these platforms disincentivizes
%        their adoption by researchers who already have the means to run
%        software simulations at low cost.
%
%    \item \textbf{FPGA Capacity.} Common ASIC structures, such as CAMs,
%        multi-ported RAMs, and wide multiplexors are known to map poorly to
%        FPGA fabrics~\cite{fpgagap, fpgagap2}, making it difficult to host
%        large target designs on an FPGA.
%
%    \item \textbf{Configurability \& Extensibility.} Extending FPGA-accelerated
%        simulations requires writing RTL. RTL models are less configurable and
%        harder to extend than software models. Finally, RTL models still need
%        to be validated, further exacerbating the challenge of building them.
%
%    \item \textbf{FPGA compile time.} Compiling an FPGA simulator considerably
%        longer than compiling a software simulator (ones of hours).
%        %To some extent this is inescapable. However, where abstract models are
%        %employed, they can be made run-time configurable, with programmable
%        %registers sitting on a simulation memory map. Where models are
%        %generated from RTL, they can can be incrementally recompiled, or perhaps
%        %partially reconfigured.
%
%    \item \textbf{Debuggability.} Debugging a broken FPGA-accelerated
%        simulation is difficult due to the limited visibility the designer has
%        over the state of the simulation. This is often more challenging than
%        debugging an FPGA prototype of the target, as FPGA-specific
%        optimizations make it more difficult to reason about the state of the
%        target.
%
%\end{enumerate}
%
%\section{Why Continue With FPGA-Accelerated Simulation?}
%
%Even as Moore's law wanes, FPGA capacity continues to scale. The largest FPGAs
%have over 50 MB of BRAM and millions of logic cells\footnote{Comically, scaling
%RAMPGold~\cite{rampgold}, to use the largest Xilinx UltraScale
%FPGA~\cite{ultrascale} by BRAM capacity would permit modeling in excess of 5000
%cores.}. As they have scaled, FPGAs have continued to become more
%heterogeneous, adding features that make them more amenable to hosting
%full-system simulaions.  Both Intel and Xilinx now sell FPGAs with embedded ARM
%cores, making it easier to co-simulate tightly coupled hardware and software
%models of a system. Modern FPGAs include hardened DRAM controllers that are
%comparable to those of ASICs. This trend towards greater integration looks to
%continue. For example, upcoming Intel Stratix 10 MX parts include in-package DRAM (HBM2)
%that can support up to 1 TBps of aggregate memory bandwidth~\cite{stratix10mx}.
%Both DRAM capacity and bandwidth are crucial for simulating components
%of the target that may not fit in BRAM.
%
%Lower cost and increased on-chip integration have also made FPGAs more
%accessible. Not only are COTS development boards cheaper and more featureful,
%FPGAs are now available in academic clusters, like TACC's Microsoft
%Catapult~\cite{catapultannounce} deployment, and in datacenters, as a service
%like Amazon Web Services' EC2 F1 instances~\cite{amazonf1}. Where in the past
%academics would have to purchase their own FPGAs -- even to reproduce published
%experiments -- it may soon be possible for them to instead spin up an identical
%simulation on a shared computing resource. Companies may no longer have to
%maintain their own FPGA prototyping clusters; they could instead batch out
%simulations to the cloud.
%
%\section{Improving Usability Through Automation}
%
%While the trends described in the previous section solve the
%\emph{availability} and \emph{FPGA-capacity} challenges, the remaining three
%usability challenges persist. Previous work~\cite{fabscalarfpga, strober} has
%shown that much of an FPGA-accelerated simulator can be automatically generated
%from source RTL. This RTL can be written in an HDL like Verilog or emitted by
%high-level synthesis tools or generators written in languages like
%Chisel~\cite{chisel}. While this still requires an RTL implementation, the same
%RTL can be passed to an EDA flow to measure physical design metrics, and,
%best of all, no validation of the generated model is required.
%
%This is, unfortunately, not a panacea: perhaps the RTL is not yet
%available, or a more abstract, reconfigurable model is desired. In this report
%we consider off-chip DRAM memory systems as a motivating example: they have too
%much state to be hosted in-fabric and yet they must must be tightly coupled to
%the processor model. This makes it difficult to co-simulate DRAM in
%software.\footnote{High-latency peripherals, like disks, can often be modeled
%in software without any performance cost~\cite{disksim}.} These components
%typically require an abstract model that virtualizes the target-memory system
%over FPGA-host-memory system.
%
%This reintroduces the aforementioned problem that anything but a simplistic RTL
%model is difficult to design, modify and reuse. We propose to address this
%through \emph{generators} that synthesize abstract memory system models that
%can be easily modified and used across a wide range of targets.  This generator
%must provide a variety of timing models so as to enable the designer to trade
%fidelity for FPGA-resource savings when needed. Generated models must be reconfigurable, to
%permit sweeping memory system parameters without needing to recompile the FPGA
%bitstream. These models must provide useful instrumentation to both aid in
%debugging and to provide insight about memory system behavior without
%perturbing execution. Finally, when the generator does not provide an
%appropriate timing model, the generator should be easy to extend.
