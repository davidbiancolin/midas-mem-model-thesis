% This section describes historical and related FPGA simulation work, and aims
% to call out techniques employed by this paper, in addition to describing
% problems with those works

We first review the use of FPGAs for architecture studies.  Throughout
this paper, we make a distinction between the \emph{target} and
the \emph{host}.  The target is the design under study.  Combining the
target with a model of the environment in which it executes forms a
determinate closed system whose behavior is defined independently of
the simulation host.  The host is the hardware that
executes~(\emph{hosts}) the simulation.  In this paper, a host
consists of one or more CPUs connected to one or more FPGAs.

\section{FPGA Prototyping}
FPGAs have long been used to \emph{prototype} ASICs by implementing
the ASIC RTL directly in FPGA logic.  While FPGA prototypes are both
fast~(10s to 100s of MHz) and detailed, they require a complete RTL
description of the target design. Furthermore, larger designs must be
painstakingly partitioned across multiple FPGAs. Since these
multi-FPGA prototypes advance in lockstep, cycle by cycle, they are
considerably slower~(100s of KHz to 1s of MHz). Nonetheless, FPGAs are used widely
in industry, as they allow software development and hardware
validation to proceed months before silicon is available.

\section{FPGA-Accelerated Simulation}

Prior work has explored techniques to make FPGAs more usable and
powerful simulation hosts.  Motivated by the dawn of the multicore
era, the multi-university RAMP project~\cite{RAMP} made large strides
in improving FPGA-accelerated simulators by improving resource
efficiency, developing FPGA partitioning techniques, and avoiding FPGA
recompilation by using reconfigurable models.

ProtoFlex~\cite{ProtoFlex} was an architecture-level simulator that
demonstrated 16-way host-multithreading of a single FPGA-hosted functional model.
ProtoFlex could switch between FPGA-hosted and CPU-hosted modes via
\emph{transplantation}. FAST~\cite{FAST}, a cycle-accurate simulator, was
split into CPU-hosted functional and FPGA-hosted timing models.  RAMP
Gold~\cite{RAMPGold} used FPGA-hosted timing and functional models
with 64-way host-multithreading to model a larger target on a single
FPGA.  HAsim~\cite{HASim} also used FPGA-hosted timing and functional
models, but provided more detailed pipeline and memory hierarchy
models.

Other work studied partitioning targets over multiple FPGAs.
\cite{LIFPGADesign} showed that by partitioning HAsim over two FPGAs, they
could model eight times as many cores, due to improved resource sharing between
virtual instances. To model a datacenter-scale target, DIABLO~\cite{Diablo}
leveraged RAMP Gold's multithreading to simulate 3072 servers on 24 FPGAs.

A unifying theme of FPGA-accelerated simulators is that one clock-cycle of
target time is executed over a variable number of FPGA-host
cycles.  This lets an FPGA-hosted simulator hide
variable host latencies to DRAM and CPU-hosted components, enables
optimizations that trade host time for host resources, and, crucially,
facilitates deterministic simulation.  This \emph{host-target
decoupling} is what differentiates an FPGA-accelerated simulator from
an FPGA prototype. We expand on this property in
Section~\ref{sec:fame1}.

\section{Adoption Challenges}

Despite their promise, FPGA-accelerated simulators have only been
successfully employed by those who designed them. We attribute their
limited appeal to several factors:

\begin{enumerate}

    \item \textbf{Availability.} Much of the early FPGA-accelerated
    simulator research relied on boutique FPGA-emulation
    platforms or custom board designs, whose high cost and
    limited availability prevented adoption.

    \item \textbf{FPGA Capacity.} Common ASIC structures, such as CAMs and
        multi-ported RAMs, map poorly to FPGA
        fabrics~\cite{FPGAGap2}, making it difficult to host large
        ASIC designs on FPGAs.

    \item \textbf{Ease of Use.} To avoid partitioning across multiple
     FPGAs, previous work focused on efficiently mapping more of the
     target to a single FPGA. The abstract, multithreaded models these
     simulators typically employ can be more difficult to implement
     than the machines they model, greatly undermining their
     usability. This complexity limits configurability, forcing users
     to modify a sophisticated piece of RTL to make larger
     changes. Furthermore, these abstract models must, like their
     software counterparts, be validated and calibrated, making them
     even more laborious to use.
\end{enumerate}

\section{Recent Technological Advances}

Even as Moore's law wanes, FPGA capacity continues to scale. The largest FPGAs
have over \wunits{50}{MiB} of BRAM and millions of logic cells.
%\footnote{Scaling RAMP
%Gold~\cite{RAMPGold} to use the largest Xilinx UltraScale+
%FPGA~\cite{Ultrascale} would permit modeling more than 5000 cores!}
As they
have scaled, FPGAs have become more heterogeneous, adding features that make
them better hosts for full-system simulators.  Both Intel and Xilinx sell FPGAs
with embedded microprocessors, making it easier to co-simulate tightly coupled
hardware and software models. Modern FPGAs include dedicated DRAM
controllers that support memory bandwidths rivaling those of ASICs.

Lower cost and increased on-chip integration have also made FPGAs more
accessible to researchers.  Not only are commercial off-the-shelf development
boards cheaper and more full-featured, FPGAs are now available as a cloud service~\cite{amazonf1}.
Where in the past academics would have to purchase their own FPGAs to reproduce
published experiments, instead, it is now possible to spin up identical
simulations on FPGAs in the cloud.  This development promises to foster more
collaboration around FPGA-accelerated simulation.

\section{Usability Through Automation}

While the trends described in the previous section solve the
\emph{availability} and \emph{FPGA capacity} challenges, usability remains a
problem. Previous work~\cite{fabscalarfpga, strober} has shown that much of an
FPGA-accelerated simulator can be automatically generated from source RTL. This RTL
can be written in an HDL like Verilog, generated by a high-level synthesis
tool, or emitted by languages like Chisel~\cite{Chisel} or Bluespec.

Alas, it is not always practical to generate models from source
RTL. Consider off-chip memory systems: they are too resource-intensive to host
in the FPGA fabric, yet for reasonable simulation performance, they must be
tightly coupled with the processor model.  Components like these require
an abstract model to virtualize the target memory system over DRAM attached
to the FPGA---reintroducing the problem that anything
but a simplistic model is difficult to design, validate, modify, and reuse.

To avoid these pitfalls, we propose writing detailed memory-system
timing models as decoupled, split-timing-and-functional models with
the timing models written as \emph{target-time} RTL. Using this
approach, the same RTL transformations applied automatically to the
processor RTL are applied to the timing-model RTL before binding it to
the functional model.  Model designers can focus on modeling detailed
target behavior and not worry about the mapping to the host.  With our
approach, since timing models are transformed from target-time RTL, it
is possible to use HLS-generated RTL or even existing
memory-controller RTL as a timing model.

To improve reusability, we propose writing timing models
as \emph{generators}.  This allows the model designer to describe a
space of \emph{instances} with less development effort.  To
support reconfiguring timing models without FPGA recompilation, timing
models expose timing parameters as I/Os that are bound automatically
to memory-mapped registers during timing-model generation.  Taken
together, these techniques make it possible to describe
detailed, reusable memory-system models.  We demonstrate this claim with \PNAME.
